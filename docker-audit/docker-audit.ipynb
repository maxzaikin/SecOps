{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820c1afe",
   "metadata": {},
   "source": [
    "# ðŸ§© Docker Microservices Security Audit Notebook\n",
    "\n",
    "**Author:** Maks Zaikin  \n",
    "**Repository:** [maxzaikin/SecOps](https://github.com/maxzaikin/SecOps/tree/main/docker-audit)  \n",
    "**Date:** {{11-13-2025}}  \n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Overview\n",
    "This Jupyter Notebook contains a structured approach to auditing the security posture of containerized microservice architectures.  \n",
    "It demonstrates how to collect, analyze, and correlate information from Docker environments to identify potential configuration and security issues.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Disclaimer\n",
    "This notebook is provided **as is**, without any warranty or guarantee of fitness for your environment.  \n",
    "Use it **at your own risk** â€” always test in a **controlled or isolated environment** before applying any method in production systems.\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ›¡ï¸ **Note:**  \n",
    "> The goal of this notebook is educational and methodological â€” to share a reproducible audit approach, not to expose or assess any specific commercial product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277de159",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb34fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def expand_network_data(df): \n",
    "    '''\n",
    "    Description: Flatten json data\n",
    "                 How to prepare data\n",
    "                 1. Inpect network docker network inspect\n",
    "                 2. Create a *.json file in VS code and place []\n",
    "                 3. r.m. click Select 'Format Document'\n",
    "    Return: \n",
    "    '''\n",
    "    \n",
    "    expanded_rows = [] \n",
    "    \n",
    "    for _, row in df.iterrows(): \n",
    "        # Basic fields\n",
    "        network_base = { \n",
    "            'network_name': row.get('Name', ''), \n",
    "            'network_id': row.get('Id', ''), \n",
    "            'network_created': row.get('Created', ''), \n",
    "            'network_scope': row.get('Scope', ''), \n",
    "            'network_driver': row.get('Driver', ''), \n",
    "            'network_is_ipv6_enabled': row.get('EnableIPv6', False), \n",
    "            'network_is_internal': row.get('Internal', False), \n",
    "            'network_is_attachable': row.get('Attachable', False), \n",
    "            'network_is_ingress': row.get('Ingress', False), \n",
    "            'netwrok_is_config_only': row.get('ConfigOnly', False) \n",
    "        } \n",
    "\n",
    "        # Explode IPAM \n",
    "        ipam = row.get('IPAM', {}) \n",
    "        \n",
    "        network_base['network_ipam_driver'] = ipam.get('Driver', '') \n",
    "        network_base['network_ipam_options'] = ipam.get('Options', None) \n",
    "\n",
    "        # Explode IPAM.Config \n",
    "        ipam_config = ipam.get('Config', [{}]) \n",
    "        \n",
    "        if ipam_config and len(ipam_config) > 0:\n",
    "            network_base['netwrok_ipam_config_subnet'] = ipam_config[0].get('Subnet', '') \n",
    "            network_base['netwrok_ipam_config_gateway'] = ipam_config[0].get('Gateway', '') \n",
    "        else: \n",
    "            network_base['netwrok_ipam_config_subnet'] = '' \n",
    "            network_base['netwrok_ipam_config_gateway'] = '' \n",
    "\n",
    "        # Explode ConfigFrom \n",
    "        config_from = row.get('ConfigFrom', {}) \n",
    "        network_base['netwrok_config_from'] = config_from.get('Network', '') \n",
    "\n",
    "        # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Options Ð¸ Labels \n",
    "        network_base['network_options'] = str(row.get('Options', {})) \n",
    "        network_base['netwrok_labels'] = str(row.get('Labels', {})) \n",
    "\n",
    "        # Process containers\n",
    "        containers = row.get('Containers', {}) \n",
    "        \n",
    "        if containers:\n",
    "            \n",
    "            for container_id, container_info in containers.items(): \n",
    "                expanded_row = network_base.copy() \n",
    "                expanded_row.update({'container_id': container_id, \n",
    "                                     'container_name': container_info.get('Name', ''), \n",
    "                                     'container_endpoint_id': container_info.get('EndpointID', ''), \n",
    "                                     'container_mac': container_info.get('MacAddress', ''), \n",
    "                                     'container_ipv4': container_info.get('IPv4Address', ''), \n",
    "                                     'container_ipv6': container_info.get('IPv6Address', '') }) \n",
    "                \n",
    "                expanded_rows.append(expanded_row) \n",
    "        else: \n",
    "            # If there is no container add only network info\n",
    "            expanded_row = network_base.copy() \n",
    "            expanded_row.update({'container_id': None, \n",
    "                                 'container_name': None, \n",
    "                                 'container_endpoint_id': None, \n",
    "                                 'container_mac': None, \n",
    "                                 'container_ipv4': None, \n",
    "                                 'container_ipv6': None }) \n",
    "            \n",
    "            expanded_rows.append(expanded_row) \n",
    "\n",
    "    # Create final dataframe\n",
    "    columns = ['network_name', \n",
    "               'network_id', \n",
    "               'network_created', \n",
    "               'network_scope', \n",
    "               'network_driver', \n",
    "               'network_is_ipv6_enabled', \n",
    "               'network_ipam_driver', \n",
    "               'network_ipam_options', \n",
    "               'netwrok_ipam_config_subnet', \n",
    "               'netwrok_ipam_config_gateway', \n",
    "               'network_is_internal', \n",
    "               'network_is_attachable', \n",
    "               'network_is_ingress', \n",
    "               'netwrok_config_from', \n",
    "               'netwrok_is_config_only', \n",
    "               'network_options', \n",
    "               'network_labels', \n",
    "               'container_id', \n",
    "               'container_name', \n",
    "               'container_endpoint_id', \n",
    "               'container_mac', \n",
    "               'container_ipv4', \n",
    "               'container_ipv6' \n",
    "              ] \n",
    "            \n",
    "    return pd.DataFrame(expanded_rows, columns=columns) \n",
    "\n",
    "\n",
    "def expand_container_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Description: Flatten `docker ps --no-trunc` JSON dump\n",
    "                 How to prepare data\n",
    "                 1. Run command: docker ps --no-trunc --format '{{json .}}'\n",
    "                 2. Create a *.json file in VS code and place []\n",
    "                 3. In between [] paste output \n",
    "                 4. r.m. click Select 'Format Document'\n",
    "                 5.Don't forget to place ',' between {container data},{containcer data}... \n",
    "    Return: pandas.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    expanded_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        base = {\n",
    "            'container_id': row.get('ID', ''),\n",
    "            'container_name': row.get('Names', ''),\n",
    "            'container_image': row.get('Image', ''),\n",
    "            'container_command': row.get('Command', ''),\n",
    "            'container_created_at': row.get('RunningFor', ''),\n",
    "            'container_status': row.get('Status', ''),\n",
    "            'container_ports_raw': row.get('Ports', ''),\n",
    "            'container_labels_raw': row.get('Labels', ''),\n",
    "            'container_mounts': row.get('Mounts', ''),\n",
    "            'container_networks': row.get('Networks', ''),\n",
    "            'container_local_names': row.get('LocalVolumes', ''),\n",
    "            'container_state': row.get('State', ''),\n",
    "            'container_size': row.get('Size', ''),\n",
    "        }\n",
    "\n",
    "        # ports parsing\n",
    "        ports = row.get('Ports', '')\n",
    "        if ports:\n",
    "            base['container_ports'] = [p.strip() for p in ports.split(',')]\n",
    "        else:\n",
    "            base['container_ports'] = []\n",
    "\n",
    "        # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¼ÐµÑ‚ÐºÐ¸ (Labels Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÑÑ‚Ñ€Ð¾ÐºÐ¾Ð¹ Ð¸Ð»Ð¸ ÑÐ»Ð¾Ð²Ð°Ñ€Ñ‘Ð¼)\n",
    "        labels = row.get('Labels', '')\n",
    "        if isinstance(labels, str) and '=' in labels:\n",
    "            label_dict = {}\n",
    "            for item in labels.split(','):\n",
    "                if '=' in item:\n",
    "                    k, v = item.split('=', 1)\n",
    "                    label_dict[k.strip()] = v.strip()\n",
    "            base['container_labels'] = label_dict\n",
    "        elif isinstance(labels, dict):\n",
    "            base['container_labels'] = labels\n",
    "        else:\n",
    "            base['container_labels'] = {}\n",
    "\n",
    "        # explode everythinf in separate fields\n",
    "        for k, v in base['container_labels'].items():\n",
    "            safe_key = 'container_label_' + k.replace('.', '_').replace('-', '_')\n",
    "            base[safe_key] = v\n",
    "\n",
    "        expanded_rows.append(base)\n",
    "\n",
    "    # Arrange cols\n",
    "    cols = [\n",
    "        'container_id',\n",
    "        'container_name',\n",
    "        'container_image',\n",
    "        'container_command',\n",
    "        'container_created_at',\n",
    "        'container_status',\n",
    "        'container_state',\n",
    "        'container_networks',\n",
    "        'container_ports_raw',\n",
    "        'container_ports',\n",
    "        'container_mounts',\n",
    "        'container_labels_raw',\n",
    "        'container_local_names',\n",
    "        'container_size'\n",
    "    ]\n",
    "\n",
    "    # Add dynamically label_* fields (if any...)\n",
    "    extra_cols = sorted({c for row in expanded_rows for c in row.keys()} - set(cols))\n",
    "    columns = cols + extra_cols\n",
    "\n",
    "    return pd.DataFrame(expanded_rows, columns=columns)\n",
    "\n",
    "def merge_network_and_container_data(network_df: pd.DataFrame,\n",
    "                                     container_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge expand_network_data() and  expand_container_data()\n",
    "    by container_id.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standardize key\n",
    "    network_df['container_id'] = network_df['container_id'].fillna('').astype(str).str[:12]\n",
    "    container_df['container_id'] = container_df['container_id'].fillna('').astype(str).str[:12]\n",
    "\n",
    "    #  merge\n",
    "    merged = pd.merge(\n",
    "        network_df,\n",
    "        container_df,\n",
    "        on='container_id',\n",
    "        how='left',\n",
    "        suffixes=('_network', '_container')\n",
    "    )\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19445383",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read data and process initial fill\n",
    "\n",
    "containers_df =pd.read_json('containers-inv.json')\n",
    "containers_df = expand_container_data(containers_df)\n",
    "\n",
    "networks1_df=pd.read_json('network1.json')\n",
    "networks1_df = expand_network_data(networks1_df)\n",
    "\n",
    "networks2_df=pd.read_json('network2.json')\n",
    "networks2_df = expand_network_data(networks2_df)\n",
    "\n",
    "nwk_df=pd.concat([networks1_df,\n",
    "                  networks2_df,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a868c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# merge all in one\n",
    "merged_df = merge_network_and_container_data(nwk_df, contaibners_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246db263",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# add extra fields\n",
    "\n",
    "merged_df['container_description']=''\n",
    "merged_df['container_ncpu']=''\n",
    "merged_df['container_nram']=''\n",
    "merged_df['container_ndisk']=''\n",
    "merged_df['container_hostname']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ad59b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# check structure\n",
    "merged_df.info()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
